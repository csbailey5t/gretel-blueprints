# For datasets that have a low number of records, in the hundreds or so.
#
# Here we set our vocab size to 0 which will automatically set 
# Gretel Synthetics to use a character based tokenizer.
#
# Privacy filtering is disabled as low record counts will lead
# to higher amounts of repeated data.

schema_version: "1.0"

models:
  - synthetics:
      data_source: __tmp__
      params:
        epochs: 100
        batch_size: 64
        vocab_size: 0
        learning_rate: 0.01
        rnn_units: 256
        dropout_rate: 0.2
        early_stopping: True
        gen_temp: 1.0
        predict_batch_size: 64
        validation_split: False
        data_upsample_limit: 10000
      validators:
        in_set_count: 10
        pattern_count: 10
      generate:
        num_records: null
        max_invalid: null
      privacy_filters:
        outliers: null
        similarity: null
