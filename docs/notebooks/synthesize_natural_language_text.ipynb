{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTRxpSlaczHY"
   },
   "source": [
    "# Generating Synthetic Text\n",
    "\n",
    "This notebook will walk you through generating synthetic natural language text, similar to the data that you provide to it. This is accomplished by fine-tuning a large scale language model that has been pre-trained on billions of documents and is therefore capable of introducing realistic new variations into the data.\n",
    " \n",
    "To run this notebook, you will need an API key from the Gretel console,  at https://console.gretel.cloud. \n",
    "\n",
    "** **Limitations and Biases** **\n",
    "Large-scale language models such as GPT-X may produce untrue and/or offensive content without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results. For more information and examples please see [OpenAI](https://huggingface.co/gpt2#limitations-and-bias) and [EleutherAI](https://huggingface.co/EleutherAI/gpt-neo-125M#limitations-and-biases)'s docs for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the project\n",
    "* Install dependencies\n",
    "* Import libraries\n",
    "* Log into Gretel and set up a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEM6kjRsczHd"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U gretel-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQ-TmAdwczHd"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from gretel_client import configure_session\n",
    "from gretel_client.helpers import poll\n",
    "from gretel_client.projects import create_or_get_unique_project, get_project\n",
    "\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into Gretel\n",
    "\n",
    "configure_session(api_key=\"prompt\", cache=\"yes\", endpoint=\"https://api-dev.gretel.cloud\", validate=True, clear=True) #clear=True\n",
    "\n",
    "project = create_or_get_unique_project(name=\"synthetic-text\")\n",
    "project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PD5B0U06ALs"
   },
   "source": [
    "## Create the model configuration\n",
    "\n",
    "In this notebook we will use GPT-Neo, a transformer model designed using EleutherAI's replication of OpenAI's GPT-3 Architecture. This model has been pre-trained on the Pile, a large-scale dataset using 300 billion tokens over 572,300 steps. In this introductory example, we will fine-tune GPT-Neo to generate synthetic (and hopefully entertaining) cocktail recipes by fine-tuning across a dataset of well known cocktail recipes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"schema_version\": 1,\n",
    "  \"models\": [\n",
    "    {\n",
    "      \"gpt_x\": {\n",
    "        \"data_source\": \"__\",\n",
    "        \"pretrained_model\": \"EleutherAI/gpt-neo-125M\",\n",
    "        \"batch_size\": 4,\n",
    "        \"epochs\": 3,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"lr_scheduler\": \"cosine\",\n",
    "        \"learning_rate\": 0.0005\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9LTh7GO6VIu"
   },
   "source": [
    "## Load and preview the text dataset\n",
    "Specify a data source to train the model on. This can be a local file, web location, or HDFS file. Currently, the text dataset must be saved in single-column CSV format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMg9nX6SczHe"
   },
   "outputs": [],
   "source": [
    "dataset_path = 'https://gretel-public-website.s3.us-west-2.amazonaws.com/datasets/drink-recipes.csv'\n",
    "df = pd.concat([pd.read_csv(dataset_path)] * 1)\n",
    "\n",
    "df.to_csv('training_data.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxnH8th-65Dh"
   },
   "source": [
    "## Train the synthetic model\n",
    "In this step, we will task the worker running in the Gretel cloud, or locally, to fine-tune the GPT language model on the source dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4-E_F0qczHe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "model = project.create_model_obj(model_config=config)\n",
    "model.data_source = \"training_data.csv\"\n",
    "model.name = \"cocktail-generator\"\n",
    "model.submit_cloud()\n",
    "\n",
    "poll(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IkWOnVQ7oo1"
   },
   "source": [
    "## Generate synthetic text data\n",
    "You can now use the fine-tuned synthetic model to generate as much synthetic data as you like. The next cells walk through three ways to generate data.\n",
    "\n",
    "1.  Generate text records from the model\n",
    "2.  Generate text records using a single text seed (or prompt)\n",
    "3.  Generate text records using a unique seed per record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0bI0OpI6W3Y"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Example 1: Generate text records from the model.\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "record_handler = model.create_record_handler_obj(\n",
    "    params={\"num_records\": 20, \"maximum_text_length\": 128}\n",
    ")\n",
    "record_handler.submit_cloud()\n",
    "poll(record_handler)\n",
    "\n",
    "pd.read_csv(record_handler.get_artifact_link(\"data\"), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Generate text with an optional \"prompt\" which is used to condition\n",
    "# model generation.\n",
    "\n",
    "record_handler = model.create_record_handler_obj(\n",
    "    params={\"num_records\": 5, \n",
    "            \"maximum_text_length\": 128,\n",
    "            \"prompt\": \"Two software engineers walk into a bar. What do they order?\"}\n",
    ")\n",
    "record_handler.submit_cloud()\n",
    "poll(record_handler)\n",
    "\n",
    "pd.read_csv(record_handler.get_artifact_link(\"data\"), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Generate text with optional \"prompts\" to condition model generation \n",
    "# with an individual prompt for each record in CSV format.\n",
    "\n",
    "prompts = pd.DataFrame([\"Can you make me a drink with orange juice in it?\"]*5, columns=[\"text\"])\n",
    "prompts.to_csv('prompts.csv', index=False)\n",
    "\n",
    "record_handler = model.create_record_handler_obj(\n",
    "    params={\"maximum_text_length\": 128},\n",
    "    data_source='prompts.csv'\n",
    ")\n",
    "record_handler.submit_cloud()\n",
    "poll(record_handler)\n",
    "\n",
    "pd.read_csv(record_handler.get_artifact_link(\"data\"), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Create synthetic data from a DataFrame or CSV",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
